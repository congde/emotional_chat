#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“åº”ç”Ÿæˆæ¨¡å—ç¤ºä¾‹ä»£ç 
Response Generation Module Examples

å±•ç¤ºå¦‚ä½•ä½¿ç”¨å“åº”ç”Ÿæˆæ¨¡å—çš„å„ç§åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from backend.modules.intent.core.response_generator import ResponseGenerator
from backend.modules.intent.core.enhanced_input_processor import EnhancedInputProcessor
from backend.emotion_analyzer import EmotionAnalyzer

try:
    from langchain_openai import ChatOpenAI
except ImportError:
    from langchain.chat_models import ChatOpenAI

from config import Config


def example_1_basic_usage():
    """ç¤ºä¾‹1ï¼šåŸºç¡€ç”¨æ³•"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹1ï¼šåŸºç¡€ç”¨æ³•")
    print("="*60 + "\n")
    
    # åˆ›å»ºLLMå®¢æˆ·ç«¯
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    # åˆ›å»ºå“åº”ç”Ÿæˆå™¨
    generator = ResponseGenerator(llm_client)
    
    # ç”Ÿæˆå›å¤
    result = generator.generate_response(
        user_input="æˆ‘ä»Šå¤©è¢«é¢†å¯¼æ‰¹è¯„äº†ï¼Œè§‰å¾—è‡ªå·±ä¸€æ— æ˜¯å¤„",
        user_emotion="sad",
        user_id="user_001",
        emotion_intensity=7.5
    )
    
    print(f"ç”¨æˆ·: æˆ‘ä»Šå¤©è¢«é¢†å¯¼æ‰¹è¯„äº†ï¼Œè§‰å¾—è‡ªå·±ä¸€æ— æ˜¯å¤„")
    print(f"æƒ…ç»ª: æ‚²ä¼¤ (å¼ºåº¦: 7.5)")
    print(f"\nå¿ƒè¯­: {result['response']}")
    print(f"\nç”Ÿæˆæ–¹æ³•: {result['generation_method']}")
    print(f"æ˜¯å¦æœ‰æ•ˆ: {result['is_valid']}")


def example_2_with_history():
    """ç¤ºä¾‹2ï¼šå¸¦å¯¹è¯å†å²"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹2ï¼šå¸¦å¯¹è¯å†å²çš„å›å¤ç”Ÿæˆ")
    print("="*60 + "\n")
    
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    generator = ResponseGenerator(llm_client)
    
    # å¯¹è¯å†å²
    conversation_history = [
        {"role": "user", "content": "æœ€è¿‘å·¥ä½œå‹åŠ›å¥½å¤§"},
        {"role": "assistant", "content": "æˆ‘èƒ½æ„Ÿå—åˆ°ä½ çš„å‹åŠ›ã€‚å·¥ä½œç¡®å®ä¸å®¹æ˜“ã€‚"},
        {"role": "user", "content": "ä»Šå¤©åˆåŠ ç­åˆ°å¾ˆæ™š"},
        {"role": "assistant", "content": "åŠ ç­ç¡®å®å¾ˆè¾›è‹¦ã€‚æ³¨æ„ä¼‘æ¯å“¦ã€‚"}
    ]
    
    # ç”Ÿæˆå›å¤ï¼ˆè€ƒè™‘å†å²ï¼‰
    result = generator.generate_response(
        user_input="æˆ‘è§‰å¾—å¿«æ’‘ä¸ä¸‹å»äº†",
        user_emotion="frustrated",
        user_id="user_001",
        emotion_intensity=8.0,
        conversation_history=conversation_history
    )
    
    print("å¯¹è¯å†å²:")
    for turn in conversation_history:
        role = "ç”¨æˆ·" if turn["role"] == "user" else "å¿ƒè¯­"
        print(f"  {role}: {turn['content']}")
    
    print(f"\nç”¨æˆ·: æˆ‘è§‰å¾—å¿«æ’‘ä¸ä¸‹å»äº†")
    print(f"æƒ…ç»ª: æ²®ä¸§ (å¼ºåº¦: 8.0)")
    print(f"\nå¿ƒè¯­: {result['response']}")


def example_3_with_memory():
    """ç¤ºä¾‹3ï¼šç»“åˆè®°å¿†æ£€ç´¢"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹3ï¼šç»“åˆç”¨æˆ·è®°å¿†çš„ä¸ªæ€§åŒ–å›å¤")
    print("="*60 + "\n")
    
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    generator = ResponseGenerator(llm_client)
    
    # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„ç”¨æˆ·è®°å¿†
    retrieved_memories = [
        {
            "content": "ç”¨æˆ·æåˆ°è‡ªå·±æ˜¯ç¨‹åºå‘˜ï¼Œç»å¸¸åŠ ç­",
            "importance": 0.8,
            "timestamp": "2025-10-15T10:00:00"
        },
        {
            "content": "ç”¨æˆ·æœ€è¿‘åœ¨å‡†å¤‡é¡¹ç›®ä¸Šçº¿ï¼Œå‹åŠ›å¾ˆå¤§",
            "importance": 0.9,
            "timestamp": "2025-10-17T15:30:00"
        }
    ]
    
    # ç”Ÿæˆå›å¤
    result = generator.generate_response(
        user_input="é¡¹ç›®ç»ˆäºä¸Šçº¿äº†ï¼Œä½†æ„Ÿè§‰å¾ˆç´¯",
        user_emotion="frustrated",
        user_id="user_001",
        emotion_intensity=6.0,
        retrieved_memories=retrieved_memories
    )
    
    print("ç›¸å…³è®°å¿†:")
    for memory in retrieved_memories:
        print(f"  - {memory['content']}")
    
    print(f"\nç”¨æˆ·: é¡¹ç›®ç»ˆäºä¸Šçº¿äº†ï¼Œä½†æ„Ÿè§‰å¾ˆç´¯")
    print(f"æƒ…ç»ª: æ²®ä¸§ (å¼ºåº¦: 6.0)")
    print(f"\nå¿ƒè¯­: {result['response']}")


def example_4_user_profile():
    """ç¤ºä¾‹4ï¼šä½¿ç”¨ç”¨æˆ·ç”»åƒ"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹4ï¼šåŸºäºç”¨æˆ·ç”»åƒçš„ä¸ªæ€§åŒ–å›å¤")
    print("="*60 + "\n")
    
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    generator = ResponseGenerator(llm_client)
    
    # ç”¨æˆ·ç”»åƒ
    user_profile = {
        "preferred_tone": "æ¸©æŸ”ã€ç¼“æ…¢",
        "avoid_topics": ["å®¶åº­"],
        "communication_style": "æ·±åº¦å€¾å¬å‹",
        "emoji_preference": "moderate"
    }
    
    # ç”Ÿæˆå›å¤
    result = generator.generate_response(
        user_input="æˆ‘æœ€è¿‘å¿ƒæƒ…ä¸å¥½",
        user_emotion="sad",
        user_id="user_001",
        emotion_intensity=5.5,
        user_profile=user_profile
    )
    
    print("ç”¨æˆ·ç”»åƒ:")
    for key, value in user_profile.items():
        print(f"  {key}: {value}")
    
    print(f"\nç”¨æˆ·: æˆ‘æœ€è¿‘å¿ƒæƒ…ä¸å¥½")
    print(f"æƒ…ç»ª: æ‚²ä¼¤ (å¼ºåº¦: 5.5)")
    print(f"\nå¿ƒè¯­: {result['response']}")


def example_5_crisis_intervention():
    """ç¤ºä¾‹5ï¼šå±æœºå¹²é¢„"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹5ï¼šå±æœºå¹²é¢„åœºæ™¯")
    print("="*60 + "\n")
    
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    generator = ResponseGenerator(llm_client)
    
    # é«˜é£é™©è¾“å…¥
    crisis_input = "æˆ‘çœŸçš„ä¸æƒ³æ´»äº†"
    
    # æ¨¡æ‹Ÿé¢„å¤„ç†ç»“æœï¼ˆåŒ…å«å±æœºå…³é”®è¯ï¼‰
    metadata = {
        "requires_crisis_intervention": True,
        "risk_keywords": ["ä¸æƒ³æ´»äº†"]
    }
    
    # ç”Ÿæˆå›å¤
    result = generator.generate_response(
        user_input=crisis_input,
        user_emotion="high_risk_depression",
        user_id="user_001",
        emotion_intensity=10.0,
        metadata=metadata
    )
    
    print(f"ç”¨æˆ·: {crisis_input}")
    print(f"âš ï¸  æ£€æµ‹åˆ°é«˜é£é™©ï¼š{metadata['risk_keywords']}")
    print(f"\nå¿ƒè¯­: {result['response']}")
    print(f"\nç”Ÿæˆæ–¹æ³•: {result['generation_method']}")


def example_6_complete_pipeline():
    """ç¤ºä¾‹6ï¼šå®Œæ•´å¤„ç†æµç¨‹"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹6ï¼šå®Œæ•´çš„å¯¹è¯å¤„ç†æµç¨‹")
    print("="*60 + "\n")
    
    # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
    input_processor = EnhancedInputProcessor()
    emotion_analyzer = EmotionAnalyzer()
    
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    response_generator = ResponseGenerator(llm_client)
    
    # ç”¨æˆ·è¾“å…¥
    raw_input = "ä»Šå¤©è¢«é¢†å¯¼éª‚äº†ï¼Œæˆ‘å¥½éš¾è¿‡å•ŠğŸ˜­"
    user_id = "user_001"
    
    print(f"åŸå§‹è¾“å…¥: {raw_input}\n")
    
    # 1. è¾“å…¥é¢„å¤„ç†
    print("1ï¸âƒ£ è¾“å…¥é¢„å¤„ç†...")
    processed = input_processor.preprocess(raw_input, user_id)
    
    if processed["blocked"]:
        print(f"   âœ— è¾“å…¥è¢«é˜»æ­¢: {processed['friendly_message']}")
        return
    
    print(f"   âœ“ æ¸…æ´—å: {processed['cleaned']}")
    print(f"   - ä¸­æ–‡å æ¯”: {processed['metadata'].get('chinese_ratio', 0):.2%}")
    print(f"   - å…³é”®è¯: {', '.join(processed['metadata'].get('keywords', [])[:5])}")
    
    # 2. æƒ…æ„Ÿåˆ†æ
    print("\n2ï¸âƒ£ æƒ…æ„Ÿåˆ†æ...")
    emotion_data = emotion_analyzer.analyze_emotion(processed["cleaned"])
    
    print(f"   âœ“ æƒ…ç»ª: {emotion_data['emotion']}")
    print(f"   - å¼ºåº¦: {emotion_data['intensity']}/10")
    
    # 3. å“åº”ç”Ÿæˆ
    print("\n3ï¸âƒ£ å“åº”ç”Ÿæˆ...")
    result = response_generator.generate_response(
        user_input=processed["cleaned"],
        user_emotion=emotion_data["emotion"],
        user_id=user_id,
        emotion_intensity=emotion_data["intensity"],
        metadata=processed["metadata"]
    )
    
    print(f"   âœ“ ç”Ÿæˆæ–¹æ³•: {result['generation_method']}")
    print(f"   - æ˜¯å¦æœ‰æ•ˆ: {result['is_valid']}")
    
    # 4. è¾“å‡ºå›å¤
    print("\n4ï¸âƒ£ æœ€ç»ˆå›å¤:")
    print(f"   å¿ƒè¯­: {result['response']}")


def example_7_statistics():
    """ç¤ºä¾‹7ï¼šç»Ÿè®¡ä¿¡æ¯"""
    print("\n" + "="*60)
    print("ç¤ºä¾‹7ï¼šæŸ¥çœ‹ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯")
    print("="*60 + "\n")
    
    llm_client = ChatOpenAI(
        api_key=Config.OPENAI_API_KEY,
        base_url=Config.API_BASE_URL,
        model_name=Config.DEFAULT_MODEL,
        temperature=0.7
    )
    
    generator = ResponseGenerator(llm_client)
    
    # ç”Ÿæˆå¤šä¸ªå›å¤
    test_cases = [
        ("ä½ å¥½", "neutral", 3.0),
        ("è°¢è°¢ä½ ", "grateful", 4.0),
        ("æˆ‘å¾ˆç„¦è™‘", "anxious", 7.0),
        ("å†è§", "neutral", 3.0)
    ]
    
    for user_input, emotion, intensity in test_cases:
        generator.generate_response(
            user_input=user_input,
            user_emotion=emotion,
            user_id="test_user",
            emotion_intensity=intensity
        )
    
    # è·å–ç»Ÿè®¡
    stats = generator.get_statistics()
    
    print("ç”Ÿæˆç»Ÿè®¡:")
    print(f"  æ€»è®¡: {stats['total_generations']} æ¬¡")
    print(f"  LLMç”Ÿæˆ: {stats['llm_generated']} æ¬¡ ({stats.get('llm_rate', 0):.2%})")
    print(f"  ç¼“å­˜åŒ¹é…: {stats['cached']} æ¬¡ ({stats.get('cached_rate', 0):.2%})")
    print(f"  è§„åˆ™å¼•æ“: {stats['rule_based']} æ¬¡ ({stats.get('rule_based_rate', 0):.2%})")
    print(f"  ä¸€è‡´æ€§å¤±è´¥: {stats['consistency_failures']} æ¬¡ ({stats.get('failure_rate', 0):.2%})")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "â•”" + "="*58 + "â•—")
    print("â•‘" + " "*15 + "å“åº”ç”Ÿæˆæ¨¡å—ç¤ºä¾‹é›†" + " "*15 + "â•‘")
    print("â•š" + "="*58 + "â•")
    
    examples = [
        ("åŸºç¡€ç”¨æ³•", example_1_basic_usage),
        ("å¸¦å¯¹è¯å†å²", example_2_with_history),
        ("ç»“åˆè®°å¿†", example_3_with_memory),
        ("ç”¨æˆ·ç”»åƒ", example_4_user_profile),
        ("å±æœºå¹²é¢„", example_5_crisis_intervention),
        ("å®Œæ•´æµç¨‹", example_6_complete_pipeline),
        ("ç»Ÿè®¡ä¿¡æ¯", example_7_statistics)
    ]
    
    print("\nå¯ç”¨ç¤ºä¾‹:")
    for i, (name, _) in enumerate(examples, 1):
        print(f"  {i}. {name}")
    print(f"  0. è¿è¡Œæ‰€æœ‰ç¤ºä¾‹")
    
    try:
        choice = input("\nè¯·é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹ (0-7): ").strip()
        
        if choice == "0":
            for name, func in examples:
                try:
                    func()
                except Exception as e:
                    print(f"\nâœ— ç¤ºä¾‹ '{name}' è¿è¡Œå¤±è´¥: {e}")
        elif choice.isdigit() and 1 <= int(choice) <= len(examples):
            idx = int(choice) - 1
            name, func = examples[idx]
            func()
        else:
            print("æ— æ•ˆçš„é€‰æ‹©")
    
    except KeyboardInterrupt:
        print("\n\nå·²å–æ¶ˆ")
    except Exception as e:
        print(f"\nâœ— è¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

