#!/usr/bin/env python3
"""
æ€§èƒ½ä¼˜åŒ–ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½
"""

import asyncio
import time
import json
import requests
from typing import Dict, List, Any
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

from backend.services.performance_optimizer import performance_optimizer
from backend.services.optimized_chat_service import optimized_chat_service
from backend.config.performance_config import performance_config


async def test_parallel_processing():
    """æµ‹è¯•å¹¶è¡Œå¤„ç†"""
    print("ğŸ”„ æµ‹è¯•å¹¶è¡Œå¤„ç†...")
    
    # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æå™¨
    class MockEmotionAnalyzer:
        async def analyze(self, text):
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return {"emotion": "positive", "intensity": 7.5}
    
    # æ¨¡æ‹Ÿå®‰å…¨æ£€æŸ¥å™¨
    class MockSafetyChecker:
        async def check(self, text):
            await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return {"safe": True, "confidence": 0.95}
    
    # æ¨¡æ‹Ÿè®°å¿†æ£€ç´¢å™¨
    class MockMemoryRetriever:
        async def retrieve(self, text):
            await asyncio.sleep(0.2)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return {"relevant_memories": ["ç”¨æˆ·ä¹‹å‰æåˆ°è¿‡ç±»ä¼¼çš„è¯é¢˜"]}
    
    # åˆ›å»ºæ¨¡æ‹Ÿå™¨
    emotion_analyzer = MockEmotionAnalyzer()
    safety_checker = MockSafetyChecker()
    memory_retriever = MockMemoryRetriever()
    
    # æµ‹è¯•å¹¶è¡Œå¤„ç†
    start_time = time.time()
    result = await performance_optimizer.parallel_processing(
        "æˆ‘ä»Šå¤©å¿ƒæƒ…å¾ˆå¥½ï¼",
        emotion_analyzer,
        safety_checker,
        memory_retriever
    )
    end_time = time.time()
    
    print(f"âœ… å¹¶è¡Œå¤„ç†å®Œæˆ")
    print(f"   å¤„ç†æ—¶é—´: {result['processing_time']:.3f}s")
    print(f"   æ€»æ—¶é—´: {end_time - start_time:.3f}s")
    print(f"   æƒ…æ„Ÿåˆ†æ: {result['emotion']}")
    print(f"   å®‰å…¨æ£€æŸ¥: {result['safety']}")
    print(f"   è®°å¿†æ£€ç´¢: {result['memory']}")


async def test_caching():
    """æµ‹è¯•ç¼“å­˜æœºåˆ¶"""
    print("\nğŸ’¾ æµ‹è¯•ç¼“å­˜æœºåˆ¶...")
    
    # æµ‹è¯•ç¼“å­˜è®¾ç½®å’Œè·å–
    test_key = "test:cache:example"
    test_value = {"message": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•ç¼“å­˜", "timestamp": time.time()}
    
    # è®¾ç½®ç¼“å­˜
    performance_optimizer.redis_client.setex(
        test_key, 
        60,  # 60ç§’TTL
        json.dumps(test_value)
    )
    
    # è·å–ç¼“å­˜
    cached = performance_optimizer.redis_client.get(test_key)
    if cached:
        cached_data = json.loads(cached)
        print(f"âœ… ç¼“å­˜è·å–æˆåŠŸ: {cached_data}")
    else:
        print("âŒ ç¼“å­˜è·å–å¤±è´¥")
    
    # æµ‹è¯•ç¼“å­˜æ€§èƒ½
    cache_key = performance_optimizer.cache_key("test", "é‡å¤çš„æµ‹è¯•å†…å®¹")
    
    # ç¬¬ä¸€æ¬¡è®¡ç®—ï¼ˆæ— ç¼“å­˜ï¼‰
    start_time = time.time()
    result1 = await performance_optimizer._get_or_compute(
        cache_key,
        lambda: asyncio.sleep(0.1) or "è®¡ç®—ç»“æœ"
    )
    first_time = time.time() - start_time
    
    # ç¬¬äºŒæ¬¡è®¡ç®—ï¼ˆæœ‰ç¼“å­˜ï¼‰
    start_time = time.time()
    result2 = await performance_optimizer._get_or_compute(
        cache_key,
        lambda: asyncio.sleep(0.1) or "è®¡ç®—ç»“æœ"
    )
    second_time = time.time() - start_time
    
    print(f"âœ… ç¼“å­˜æ€§èƒ½æµ‹è¯•")
    print(f"   é¦–æ¬¡è®¡ç®—æ—¶é—´: {first_time:.3f}s")
    print(f"   ç¼“å­˜å‘½ä¸­æ—¶é—´: {second_time:.3f}s")
    print(f"   æ€§èƒ½æå‡: {first_time/second_time:.1f}x")


async def test_streaming_response():
    """æµ‹è¯•æµå¼å“åº”"""
    print("\nğŸŒŠ æµ‹è¯•æµå¼å“åº”...")
    
    # æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯
    class MockLLMClient:
        async def stream(self, prompt):
            words = ["ä½ å¥½", "ï¼Œ", "æˆ‘", "æ˜¯", "ä½ ", "çš„", "AI", "åŠ©æ‰‹", "ï¼Œ", "å¾ˆ", "é«˜å…´", "ä¸º", "ä½ ", "æœåŠ¡", "ï¼"]
            for word in words:
                yield word
                await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
    
    mock_llm = MockLLMClient()
    
    print("   æµå¼è¾“å‡º:")
    async for chunk in performance_optimizer.stream_response("æµ‹è¯•æç¤º", mock_llm):
        if chunk.startswith("data: "):
            token = chunk[6:].strip()
            if token and token != "[DONE]":
                print(f"   {token}", end="", flush=True)
    
    print("\nâœ… æµå¼å“åº”æµ‹è¯•å®Œæˆ")


async def test_fallback_strategy():
    """æµ‹è¯•é™çº§ç­–ç•¥"""
    print("\nğŸ›¡ï¸ æµ‹è¯•é™çº§ç­–ç•¥...")
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„é™çº§ç­–ç•¥
    error_types = [
        "llm_timeout",
        "memory_timeout", 
        "vector_error",
        "general_error"
    ]
    
    for error_type in error_types:
        fallback_response = performance_optimizer.fallback_strategy(
            error_type, "ç”¨æˆ·è¾“å…¥"
        )
        print(f"   {error_type}: {fallback_response}")
    
    print("âœ… é™çº§ç­–ç•¥æµ‹è¯•å®Œæˆ")


async def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
    print("\nğŸ“Š æµ‹è¯•æ€§èƒ½ç›‘æ§...")
    
    # è·å–æ€§èƒ½æŒ‡æ ‡
    metrics = performance_optimizer.get_performance_metrics()
    print(f"   æ€§èƒ½æŒ‡æ ‡: {json.dumps(metrics, indent=2, ensure_ascii=False)}")
    
    # æµ‹è¯•æ€§èƒ½ç›‘æ§è£…é¥°å™¨
    @performance_optimizer.performance_monitor
    async def test_function(delay: float):
        await asyncio.sleep(delay)
        return "æµ‹è¯•å®Œæˆ"
    
    # æµ‹è¯•æ­£å¸¸æ‰§è¡Œ
    result = await test_function(0.1)
    print(f"   æ­£å¸¸æ‰§è¡Œç»“æœ: {result}")
    
    # æµ‹è¯•æ…¢æŸ¥è¯¢
    result = await test_function(0.5)
    print(f"   æ…¢æŸ¥è¯¢ç»“æœ: {result}")
    
    print("âœ… æ€§èƒ½ç›‘æ§æµ‹è¯•å®Œæˆ")


async def test_optimized_chat_service():
    """æµ‹è¯•ä¼˜åŒ–çš„èŠå¤©æœåŠ¡"""
    print("\nğŸ¤– æµ‹è¯•ä¼˜åŒ–çš„èŠå¤©æœåŠ¡...")
    
    # æ¨¡æ‹ŸèŠå¤©è¯·æ±‚
    chat_request = {
        "message": "æˆ‘ä»Šå¤©å¿ƒæƒ…ä¸å¤ªå¥½ï¼Œèƒ½ç»™æˆ‘ä¸€äº›å»ºè®®å—ï¼Ÿ",
        "session_id": "test_session_001",
        "user_id": "test_user_001"
    }
    
    try:
        # æµ‹è¯•ä¼˜åŒ–çš„èŠå¤©å¤„ç†
        start_time = time.time()
        response = await optimized_chat_service.chat_optimized(chat_request)
        end_time = time.time()
        
        print(f"âœ… ä¼˜åŒ–èŠå¤©å¤„ç†å®Œæˆ")
        print(f"   å¤„ç†æ—¶é—´: {end_time - start_time:.3f}s")
        print(f"   å“åº”: {response.get('response', 'æ— å“åº”')[:100]}...")
        print(f"   ä¼˜åŒ–å¯ç”¨: {response.get('optimization_enabled', False)}")
        
    except Exception as e:
        print(f"âŒ èŠå¤©æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")


async def test_health_check():
    """æµ‹è¯•å¥åº·æ£€æŸ¥"""
    print("\nğŸ¥ æµ‹è¯•å¥åº·æ£€æŸ¥...")
    
    try:
        health_status = await optimized_chat_service.health_check_optimized()
        print(f"   å¥åº·çŠ¶æ€: {health_status['status']}")
        print(f"   æ£€æŸ¥ç»“æœ: {json.dumps(health_status['checks'], indent=2, ensure_ascii=False)}")
        
    except Exception as e:
        print(f"âŒ å¥åº·æ£€æŸ¥å¤±è´¥: {e}")


async def test_configuration():
    """æµ‹è¯•é…ç½®ç®¡ç†"""
    print("\nâš™ï¸ æµ‹è¯•é…ç½®ç®¡ç†...")
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    all_config = performance_config.get_all_config()
    print("   å½“å‰é…ç½®:")
    for category, config in all_config.items():
        print(f"   {category}: {config}")
    
    # éªŒè¯é…ç½®
    is_valid = performance_config.validate_config()
    print(f"   é…ç½®æœ‰æ•ˆæ€§: {'âœ… æœ‰æ•ˆ' if is_valid else 'âŒ æ— æ•ˆ'}")


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–åŠŸèƒ½æµ‹è¯•")
    print("=" * 60)
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        await test_parallel_processing()
        await test_caching()
        await test_streaming_response()
        await test_fallback_strategy()
        await test_performance_monitoring()
        await test_optimized_chat_service()
        await test_health_check()
        await test_configuration()
        
        print("\n" + "=" * 60)
        print("âœ… æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())
