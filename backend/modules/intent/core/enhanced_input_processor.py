"""
å¢å¼ºç‰ˆè¾“å…¥é¢„å¤„ç†å™¨ - é›†æˆé”™åˆ«å­—çº æ­£ã€åˆ†è¯ã€é‡å¤æ£€æµ‹ç­‰åŠŸèƒ½
Enhanced Input Processor with typo correction, tokenization, duplicate detection

åŠŸèƒ½ç‰¹æ€§ï¼š
- é”™åˆ«å­—/ç½‘ç»œç”¨è¯­è‡ªåŠ¨çº æ­£
- åˆ†è¯ä¸è¯æ€§æ ‡æ³¨ï¼ˆjiebaï¼‰
- é‡å¤è¾“å…¥æ£€æµ‹
- é—®å¥ç±»å‹è¯†åˆ«
- è¯­è¨€æ¯”ä¾‹æ£€æµ‹
- å‹å¥½çš„é”™è¯¯æç¤º
"""

import re
import logging
from typing import Dict, List, Optional, Tuple, Any
from collections import deque
from datetime import datetime

logger = logging.getLogger(__name__)

# å°è¯•å¯¼å…¥jiebaï¼ˆå¯é€‰ä¾èµ–ï¼‰
try:
    import jieba
    import jieba.posseg as pseg
    JIEBA_AVAILABLE = True
    logger.info("âœ“ jiebaåˆ†è¯å¼•æ“å¯ç”¨")
except ImportError:
    JIEBA_AVAILABLE = False
    logger.warning("âš  jiebaæœªå®‰è£…ï¼Œåˆ†è¯åŠŸèƒ½å°†è¢«ç¦ç”¨")


class EnhancedInputProcessor:
    """å¢å¼ºç‰ˆè¾“å…¥é¢„å¤„ç†å™¨"""
    
    # å¸¸è§ç½‘ç»œç”¨è¯­/é”™åˆ«å­—æ˜ å°„è¡¨
    TYPO_MAP = {
        # ç½‘ç»œæµè¡Œè¯­
        "ç´¯è§‰ä¸çˆ±": "ç´¯è§‰ä¸çˆ±äº†",
        "è“ç˜¦é¦™è‡": "éš¾å—æƒ³å“­",
        "æˆ‘è£‚å¼€äº†": "æˆ‘å¿ƒæ€å´©äº†",
        "emoäº†": "æƒ…ç»ªä¸å¥½äº†",
        "emo": "æƒ…ç»ªä¸å¥½",
        "ç ´é˜²äº†": "å¿ƒç†é˜²çº¿è¢«å‡»ç ´äº†",
        "çˆ·é’å›": "çˆ·çš„é’æ˜¥å›æ¥äº†",
        "ç¤¾æ­»": "ç¤¾ä¼šæ€§æ­»äº¡",
        "yyds": "æ°¸è¿œçš„ç¥",
        "ç»ç»å­": "éå¸¸å¥½",
        "æ “Q": "è°¢è°¢ä½ ",
        
        # å¸¸è§é”™åˆ«å­—
        "åœ¨å—": "åœ¨å—",
        "ä½ å¥½å‘€": "ä½ å¥½",
        "æ€ä¹ˆåŠå‘€": "æ€ä¹ˆåŠ",
        "å¥½éš¾å—å•Š": "å¥½éš¾å—",
        "ç¡ä¸ç€è§‰": "ç¡ä¸ç€",
        "å¤ªç³Ÿç³•äº†": "å¤ªç³Ÿç³•",
        "æˆ‘å¾ˆç„¦è™‘": "æˆ‘å¾ˆç„¦è™‘",
        
        # æƒ…æ„Ÿè¡¨è¾¾ç®€å†™
        "éš¾è¿‡ing": "æ­£åœ¨éš¾è¿‡",
        "å¼€å¿ƒing": "æ­£åœ¨å¼€å¿ƒ",
        "ç„¦è™‘ing": "æ­£åœ¨ç„¦è™‘",
        
        # å¯æ ¹æ®å®é™…ä½¿ç”¨æƒ…å†µç»§ç»­æ·»åŠ 
    }
    
    # é«˜é£é™©è¯æ±‡ï¼ˆå±æœºå¹²é¢„ï¼‰
    HIGH_RISK_KEYWORDS = [
        "è‡ªæ€", "è‡ªæ®‹", "å‰²è…•", "è·³æ¥¼", "æœè¯", "äº†ç»“",
        "ä¸æƒ³æ´»", "æƒ³æ­»", "ç»“æŸç”Ÿå‘½", "æ’‘ä¸ä¸‹å»", 
        "æ´»ä¸ä¸‹å»", "æƒ³è‡ªæ€", "è½»ç”Ÿ", "è‡ªå°½"
    ]
    
    # æ•æ„Ÿè¯æ±‡ï¼ˆæ ¹æ®éœ€è¦é…ç½®ï¼Œè¿™é‡Œé¢„ç•™æ¥å£ï¼‰
    SENSITIVE_WORDS = [
        # å¯ä»¥æ·»åŠ éœ€è¦è¿‡æ»¤çš„æ•æ„Ÿè¯
        # æ³¨æ„ï¼šå¿ƒç†å¥åº·åœºæ™¯ä¸‹è¦è°¨æ…è¿‡æ»¤ï¼Œé¿å…å½±å“ç”¨æˆ·è¡¨è¾¾
    ]
    
    # é…ç½®å‚æ•°
    MAX_LENGTH = 500  # å•æ¬¡è¾“å…¥æœ€å¤§é•¿åº¦ï¼ˆå»ºè®®å€¼ï¼‰
    ABSOLUTE_MAX_LENGTH = 2000  # ç»å¯¹æœ€å¤§é•¿åº¦ï¼ˆç¡¬é™åˆ¶ï¼‰
    MIN_LENGTH = 1    # æœ€å°æœ‰æ•ˆé•¿åº¦
    
    def __init__(self, enable_jieba: bool = True, enable_duplicate_check: bool = True):
        """
        åˆå§‹åŒ–å¢å¼ºç‰ˆå¤„ç†å™¨
        
        Args:
            enable_jieba: æ˜¯å¦å¯ç”¨jiebaåˆ†è¯ï¼ˆå¯èƒ½å½±å“æ€§èƒ½ï¼‰
            enable_duplicate_check: æ˜¯å¦å¯ç”¨é‡å¤æ£€æµ‹
        """
        self.enable_jieba = enable_jieba and JIEBA_AVAILABLE
        self.enable_duplicate_check = enable_duplicate_check
        
        # åˆå§‹åŒ–jiebaï¼ˆå¯é€‰ï¼‰
        if self.enable_jieba:
            try:
                # é¢„åŠ è½½jiebaè¯å…¸ï¼ˆæå‡é¦–æ¬¡åˆ†è¯é€Ÿåº¦ï¼‰
                jieba.initialize()
                # æ·»åŠ è‡ªå®šä¹‰è¯æ±‡
                jieba.add_word('ç„¦è™‘', freq=1000, tag='n')
                jieba.add_word('æŠ‘éƒ', freq=1000, tag='n')
                jieba.add_word('å¤±çœ ', freq=1000, tag='n')
                jieba.add_word('å‹åŠ›å¤§', freq=1000, tag='a')
                logger.info("âœ“ jiebaåˆ†è¯å¼•æ“å·²åˆå§‹åŒ–å¹¶åŠ è½½è‡ªå®šä¹‰è¯å…¸")
            except Exception as e:
                logger.warning(f"jiebaåˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ç¦ç”¨åˆ†è¯åŠŸèƒ½")
                self.enable_jieba = False
        
        # ç”¨äºæ£€æµ‹é‡å¤è¾“å…¥çš„å†å²è®°å½•ï¼ˆæ¯ä¸ªç”¨æˆ·ç‹¬ç«‹ï¼‰
        self.user_history = {}  # {user_id: deque([msg1, msg2, ...], maxlen=10)}
        
        logger.info(f"âœ“ å¢å¼ºç‰ˆè¾“å…¥å¤„ç†å™¨å·²åˆå§‹åŒ– (jieba={self.enable_jieba}, duplicate_check={self.enable_duplicate_check})")
    
    def preprocess(self, text: str, user_id: Optional[str] = None) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é¢„å¤„ç†æµç¨‹
        
        Args:
            text: åŸå§‹è¾“å…¥æ–‡æœ¬
            user_id: ç”¨æˆ·IDï¼ˆç”¨äºé‡å¤æ£€æµ‹ï¼‰
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - original: åŸå§‹æ–‡æœ¬
            - cleaned: æ¸…æ´—åçš„æ–‡æœ¬
            - blocked: æ˜¯å¦è¢«é˜»æ­¢
            - risk_level: é£é™©ç­‰çº§ (low/medium/high)
            - warnings: è­¦å‘Šä¿¡æ¯åˆ—è¡¨
            - friendly_message: å‹å¥½çš„æç¤ºä¿¡æ¯ï¼ˆå¦‚æœæœ‰é—®é¢˜ï¼‰
            - metadata: å…ƒæ•°æ®ï¼ˆé•¿åº¦ã€åˆ†è¯ã€é—®å¥ç±»å‹ç­‰ï¼‰
        """
        result = {
            "original": text,
            "cleaned": "",
            "blocked": False,
            "risk_level": "low",
            "warnings": [],
            "friendly_message": None,
            "metadata": {}
        }
        
        # === ç¬¬1æ­¥ï¼šå»é™¤é¦–å°¾ç©ºæ ¼ä¸ç‰¹æ®Šç¬¦å· ===
        cleaned = text.strip().replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
        cleaned = re.sub(r'\s+', ' ', cleaned)  # å¤šä¸ªç©ºæ ¼åˆå¹¶ä¸ºä¸€ä¸ª
        
        # === ç¬¬2æ­¥ï¼šæ£€æŸ¥ç©ºè¾“å…¥ ===
        if not cleaned:
            result["blocked"] = True
            result["warnings"].append("è¾“å…¥ä¸ºç©º")
            result["friendly_message"] = "ä½ å¥½åƒè¿˜æ²¡è¯´è¯å‘¢~ ğŸ˜Š"
            return result
        
        # === ç¬¬3æ­¥ï¼šé•¿åº¦æ£€æŸ¥ ===
        length = len(cleaned)
        result["metadata"]["length"] = length
        
        if length > self.ABSOLUTE_MAX_LENGTH:
            # è¶…è¿‡ç»å¯¹æœ€å¤§é•¿åº¦ï¼Œå¼ºåˆ¶æˆªæ–­
            result["warnings"].append(f"è¾“å…¥æ–‡æœ¬è¿‡é•¿ï¼ˆå·²æˆªæ–­è‡³{self.ABSOLUTE_MAX_LENGTH}å­—ç¬¦ï¼‰")
            result["friendly_message"] = f"æ¶ˆæ¯å¤ªé•¿å•¦ï¼å·²è‡ªåŠ¨æˆªæ–­åˆ°{self.ABSOLUTE_MAX_LENGTH}å­—ï¼Œå»ºè®®åˆ†æ¬¡å‘é€å“¦~ ğŸ“"
            cleaned = cleaned[:self.ABSOLUTE_MAX_LENGTH]
        elif length > self.MAX_LENGTH:
            # è¶…è¿‡å»ºè®®é•¿åº¦ï¼Œä½†ä¸æˆªæ–­ï¼Œåªæç¤º
            result["warnings"].append(f"è¾“å…¥æ–‡æœ¬è¾ƒé•¿ï¼ˆ{length}å­—ç¬¦ï¼‰")
            result["metadata"]["length_warning"] = True
        
        # === ç¬¬4æ­¥ï¼šçº æ­£å¸¸è§é”™åˆ«å­—/ç½‘ç»œç”¨è¯­ ===
        original_cleaned = cleaned
        cleaned = self._correct_typos(cleaned)
        if cleaned != original_cleaned:
            result["metadata"]["typos_corrected"] = True
        
        # === ç¬¬5æ­¥ï¼šæ£€æŸ¥é‡å¤å‘é€ ===
        if self.enable_duplicate_check and user_id:
            is_repeat, repeat_count = self._check_duplicate(cleaned, user_id)
            if is_repeat:
                result["warnings"].append(f"æ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼ˆè¿ç»­{repeat_count}æ¬¡ï¼‰")
                result["metadata"]["is_duplicate"] = True
                result["metadata"]["duplicate_count"] = repeat_count
                
                if repeat_count >= 3:
                    # è¿ç»­é‡å¤3æ¬¡ä»¥ä¸Šï¼Œå¯èƒ½éœ€è¦ç‰¹åˆ«å…³æ³¨
                    result["friendly_message"] = "æˆ‘å·²ç»æ”¶åˆ°ä½ çš„æ¶ˆæ¯äº†ï¼Œæ­£åœ¨è®¤çœŸæ€è€ƒæ€ä¹ˆå›åº”~ ğŸ’­"
                    result["metadata"]["high_frequency_repeat"] = True
        
        # === ç¬¬6æ­¥ï¼šåˆ†è¯ä¸è¯æ€§æ ‡æ³¨ï¼ˆå¯é€‰ï¼‰===
        if self.enable_jieba:
            try:
                words = jieba.lcut(cleaned)
                result["metadata"]["words"] = words
                result["metadata"]["word_count"] = len(words)
                
                # æå–å…³é”®è¯ï¼ˆé¢‘ç‡è¾ƒé«˜ä¸”æœ‰æ„ä¹‰çš„è¯ï¼‰
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå¯ä»¥åç»­ä¼˜åŒ–
                meaningful_words = [w for w in words if len(w) > 1 and not re.match(r'^[\W_]+$', w)]
                result["metadata"]["keywords"] = meaningful_words[:10]  # æœ€å¤š10ä¸ª
            except Exception as e:
                logger.warning(f"åˆ†è¯å¤±è´¥: {e}")
        
        # === ç¬¬7æ­¥ï¼šè¯†åˆ«é—®å¥ç±»å‹ ===
        is_question = self._is_question(cleaned)
        result["metadata"]["contains_question"] = is_question
        
        if is_question:
            question_type = self._detect_question_type(cleaned)
            result["metadata"]["question_type"] = question_type
        
        # === ç¬¬8æ­¥ï¼šè¯­è¨€æ£€æµ‹ï¼ˆä¸­æ–‡ä¸ºä¸»ï¼‰===
        chinese_ratio = self._calculate_chinese_ratio(cleaned)
        result["metadata"]["chinese_ratio"] = round(chinese_ratio, 2)
        
        if chinese_ratio < 0.3 and length > 10:  # ä¸­æ–‡å æ¯”è¿‡ä½ä¸”æ–‡æœ¬è¾ƒé•¿
            result["warnings"].append(f"éä¸­æ–‡å†…å®¹è¾ƒå¤šï¼ˆä¸­æ–‡å æ¯”{chinese_ratio:.1%}ï¼‰")
            result["friendly_message"] = "æˆ‘æ›´æ“…é•¿ä¸­æ–‡äº¤æµå“¦ï¼Œå¦‚æœæ–¹ä¾¿çš„è¯å¯ä»¥ç”¨ä¸­æ–‡å‘Šè¯‰æˆ‘å—ï¼Ÿ ğŸŒ¸"
            result["metadata"]["low_chinese_ratio"] = True
        
        # === ç¬¬9æ­¥ï¼šé«˜é£é™©å†…å®¹æ£€æµ‹ï¼ˆæœ€é‡è¦ï¼‰===
        is_high_risk, risk_keywords = self._check_high_risk(cleaned)
        if is_high_risk:
            result["risk_level"] = "high"
            result["warnings"].append("æ£€æµ‹åˆ°é«˜é£é™©å†…å®¹")
            result["metadata"]["risk_keywords"] = risk_keywords
            result["metadata"]["requires_crisis_intervention"] = True
            logger.warning(f"âš ï¸ é«˜é£é™©è¾“å…¥ [user={user_id}]: {cleaned[:50]}... | å…³é”®è¯: {risk_keywords}")
        
        # === ç¬¬10æ­¥ï¼šæ•æ„Ÿè¯è¿‡æ»¤ ===
        filtered_text, filtered_words = self._filter_sensitive_words(cleaned)
        if filtered_words:
            result["warnings"].append(f"å·²è¿‡æ»¤{len(filtered_words)}ä¸ªæ•æ„Ÿè¯")
            result["metadata"]["filtered_words"] = filtered_words
            cleaned = filtered_text
        
        # === ç¬¬11æ­¥ï¼šæ£€æŸ¥æ˜¯å¦åªåŒ…å«ç‰¹æ®Šå­—ç¬¦ ===
        if re.match(r'^[\W_]+$', cleaned):
            result["blocked"] = True
            result["warnings"].append("è¾“å…¥å†…å®¹æ— æ•ˆï¼ˆä»…åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼‰")
            result["friendly_message"] = "ä¼¼ä¹æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆçš„å†…å®¹ï¼Œæ¢ä¸ªæ–¹å¼è¡¨è¾¾å§~ ğŸŒŸ"
            return result
        
        # === ç¬¬12æ­¥ï¼šæœ€ç»ˆæ¸…æ´—ç»“æœ ===
        result["cleaned"] = cleaned
        
        return result
    
    def _correct_typos(self, text: str) -> str:
        """
        çº æ­£å¸¸è§é”™åˆ«å­—å’Œç½‘ç»œç”¨è¯­
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            çº æ­£åçš„æ–‡æœ¬
        """
        corrected = text
        corrections_made = []
        
        for typo, correct in self.TYPO_MAP.items():
            if typo in corrected:
                corrected = corrected.replace(typo, correct)
                corrections_made.append(f"'{typo}' â†’ '{correct}'")
        
        if corrections_made:
            logger.debug(f"æ–‡æœ¬çº æ­£: {', '.join(corrections_made)}")
        
        return corrected
    
    def _check_duplicate(self, text: str, user_id: str) -> Tuple[bool, int]:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤å†…å®¹
        
        Args:
            text: å½“å‰è¾“å…¥
            user_id: ç”¨æˆ·ID
            
        Returns:
            (æ˜¯å¦é‡å¤, è¿ç»­é‡å¤æ¬¡æ•°)
        """
        # åˆå§‹åŒ–ç”¨æˆ·å†å²
        if user_id not in self.user_history:
            self.user_history[user_id] = deque(maxlen=10)
        
        history = self.user_history[user_id]
        
        # æ£€æŸ¥æœ€è¿‘çš„æ¶ˆæ¯ä¸­æ˜¯å¦æœ‰é‡å¤
        recent_messages = list(history)
        
        # è®¡ç®—è¿ç»­é‡å¤æ¬¡æ•°
        repeat_count = 0
        for msg in reversed(recent_messages):
            if msg == text:
                repeat_count += 1
            else:
                break
        
        is_duplicate = repeat_count > 0
        
        # æ·»åŠ åˆ°å†å²
        history.append(text)
        
        return is_duplicate, repeat_count
    
    def _is_question(self, text: str) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºé—®å¥
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            æ˜¯å¦ä¸ºé—®å¥
        """
        question_markers = [
            '?', 'ï¼Ÿ', 'å—', 'å‘¢', 'ä¹ˆ', 'å˜›', 'å•Š',
            'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'ä¸ºå•¥', 'å’‹', 'å¦‚ä½•', 'æ€æ ·',
            'æ˜¯ä¸æ˜¯', 'å¯¹ä¸å¯¹', 'å¥½ä¸å¥½', 'å¯ä»¥å—', 'è¡Œå—'
        ]
        
        for marker in question_markers:
            if marker in text:
                return True
        
        return False
    
    def _detect_question_type(self, text: str) -> Optional[str]:
        """
        æ£€æµ‹é—®å¥ç±»å‹
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            é—®å¥ç±»å‹ï¼š
            - how: æ€æ ·ç±»ï¼ˆå¯»æ±‚æ–¹æ³•ï¼‰
            - why: ä¸ºä½•ç±»ï¼ˆå¯»æ±‚åŸå› ï¼‰
            - what: ä»€ä¹ˆç±»ï¼ˆå¯»æ±‚ä¿¡æ¯ï¼‰
            - confirm: ç¡®è®¤ç±»ï¼ˆæ˜¯å¦/å¯¹é”™ï¼‰
            - other: å…¶ä»–é—®å¥
        """
        if not self._is_question(text):
            return None
        
        # æ€æ ·ç±»é—®å¥ï¼ˆæœ€å¸¸è§ï¼Œå¯»æ±‚å»ºè®®ï¼‰
        if any(word in text for word in ['æ€ä¹ˆåŠ', 'æ€ä¹ˆ', 'æ€æ ·', 'å¦‚ä½•', 'å’‹åŠ', 'å’‹æ•´', 'è¯¥å’‹']):
            return "how"
        
        # ä¸ºä»€ä¹ˆç±»é—®å¥ï¼ˆå¯»æ±‚åŸå› è§£é‡Šï¼‰
        if any(word in text for word in ['ä¸ºä»€ä¹ˆ', 'ä¸ºå•¥', 'ä¸ºä½•', 'å’‹å›äº‹', 'æ€ä¹ˆå›äº‹']):
            return "why"
        
        # ä»€ä¹ˆç±»é—®å¥ï¼ˆå¯»æ±‚å…·ä½“ä¿¡æ¯ï¼‰
        if any(word in text for word in ['ä»€ä¹ˆ', 'å•¥', 'å“ª', 'è°', 'å‡ ']):
            return "what"
        
        # ç¡®è®¤ç±»é—®å¥ï¼ˆå¯»æ±‚è‚¯å®šæˆ–å¦å®šï¼‰
        if any(word in text for word in ['æ˜¯ä¸æ˜¯', 'å¯¹ä¸å¯¹', 'å¥½ä¸å¥½', 'å—', 'å‘¢', 'å¯ä»¥å—', 'è¡Œå—']):
            return "confirm"
        
        return "other"
    
    def _calculate_chinese_ratio(self, text: str) -> float:
        """
        è®¡ç®—ä¸­æ–‡å­—ç¬¦å æ¯”
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼ˆ0-1ï¼‰
        """
        if not text:
            return 0.0
        
        # ç»Ÿè®¡ä¸­æ–‡å­—ç¬¦æ•°é‡ï¼ˆåŒ…æ‹¬æ ‡ç‚¹ç¬¦å·ï¼‰
        chinese_count = sum(1 for char in text if '\u4e00' <= char <= '\u9fff')
        
        return chinese_count / len(text)
    
    def _check_high_risk(self, text: str) -> Tuple[bool, List[str]]:
        """
        æ£€æŸ¥é«˜é£é™©å…³é”®è¯ï¼ˆå±æœºå¹²é¢„ç›¸å…³ï¼‰
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            (æ˜¯å¦é«˜é£é™©, åŒ¹é…çš„å…³é”®è¯åˆ—è¡¨)
        """
        text_lower = text.lower()
        matched_keywords = []
        
        for keyword in self.HIGH_RISK_KEYWORDS:
            if keyword in text_lower:
                matched_keywords.append(keyword)
        
        return len(matched_keywords) > 0, matched_keywords
    
    def _filter_sensitive_words(self, text: str) -> Tuple[str, List[str]]:
        """
        è¿‡æ»¤æ•æ„Ÿè¯ï¼ˆä½¿ç”¨æ˜Ÿå·æ›¿æ¢ï¼‰
        
        Args:
            text: æ–‡æœ¬
            
        Returns:
            (è¿‡æ»¤åçš„æ–‡æœ¬, è¢«è¿‡æ»¤çš„è¯åˆ—è¡¨)
        """
        filtered = text
        filtered_words = []
        
        for word in self.SENSITIVE_WORDS:
            if word in filtered:
                filtered = filtered.replace(word, "*" * len(word))
                filtered_words.append(word)
        
        return filtered, filtered_words
    
    def validate_input(self, text: str) -> Tuple[bool, Optional[str]]:
        """
        å¿«é€ŸéªŒè¯è¾“å…¥æ˜¯å¦åˆè§„ï¼ˆè½»é‡çº§æ£€æŸ¥ï¼‰
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            (æ˜¯å¦åˆè§„, å‹å¥½çš„é”™è¯¯æç¤º)
        """
        if not text or not text.strip():
            return False, "ä½ å¥½åƒè¿˜æ²¡è¯´è¯å‘¢~ ğŸ˜Š"
        
        if len(text) > 5000:
            return False, "æ¶ˆæ¯å¤ªé•¿å•¦ï¼å»ºè®®åˆ†æˆå‡ æ¬¡å‘é€~ ğŸ“"
        
        # æ£€æŸ¥æ˜¯å¦åªåŒ…å«ç‰¹æ®Šå­—ç¬¦
        cleaned = text.strip()
        if re.match(r'^[\W_]+$', cleaned):
            return False, "ä¼¼ä¹æ²¡æœ‰è¯†åˆ«åˆ°æœ‰æ•ˆçš„å†…å®¹ï¼Œæ¢ä¸ªæ–¹å¼è¡¨è¾¾å§~ ğŸŒŸ"
        
        return True, None
    
    def clear_user_history(self, user_id: str):
        """
        æ¸…é™¤ç”¨æˆ·çš„è¾“å…¥å†å²
        
        Args:
            user_id: ç”¨æˆ·ID
        """
        if user_id in self.user_history:
            del self.user_history[user_id]
            logger.info(f"å·²æ¸…é™¤ç”¨æˆ· {user_id} çš„è¾“å…¥å†å²")
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return {
            "jieba_enabled": self.enable_jieba,
            "duplicate_check_enabled": self.enable_duplicate_check,
            "tracked_users": len(self.user_history),
            "max_length": self.MAX_LENGTH,
            "absolute_max_length": self.ABSOLUTE_MAX_LENGTH,
            "typo_rules": len(self.TYPO_MAP),
            "high_risk_keywords": len(self.HIGH_RISK_KEYWORDS),
            "sensitive_words": len(self.SENSITIVE_WORDS)
        }
    
    def add_typo_rule(self, typo: str, correct: str):
        """
        åŠ¨æ€æ·»åŠ é”™åˆ«å­—çº æ­£è§„åˆ™
        
        Args:
            typo: é”™è¯¯å†™æ³•
            correct: æ­£ç¡®å†™æ³•
        """
        self.TYPO_MAP[typo] = correct
        logger.info(f"æ·»åŠ çº é”™è§„åˆ™: '{typo}' â†’ '{correct}'")
    
    def add_high_risk_keyword(self, keyword: str):
        """
        åŠ¨æ€æ·»åŠ é«˜é£é™©å…³é”®è¯
        
        Args:
            keyword: å…³é”®è¯
        """
        if keyword not in self.HIGH_RISK_KEYWORDS:
            self.HIGH_RISK_KEYWORDS.append(keyword)
            logger.info(f"æ·»åŠ é«˜é£é™©å…³é”®è¯: '{keyword}'")


# åˆ›å»ºå…¨å±€å•ä¾‹ï¼ˆå¯é€‰ï¼‰
_global_processor = None

def get_global_processor() -> EnhancedInputProcessor:
    """è·å–å…¨å±€å¤„ç†å™¨å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _global_processor
    if _global_processor is None:
        _global_processor = EnhancedInputProcessor()
    return _global_processor

