#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“åº”ç”Ÿæˆå™¨ - æ ¸å¿ƒæ¨¡å—
Response Generator Core Module

åŠŸèƒ½ï¼š
- åŸºäºæƒ…æ„Ÿæ„å›¾åŠ¨æ€ç”ŸæˆAIå›å¤
- èåˆè§„åˆ™å¼•æ“ã€ç¼“å­˜åŒ¹é…å’ŒLLMç”Ÿæˆçš„æ··åˆæ¶æ„
- å®ç°æƒ…æ„Ÿä¸€è‡´æ€§æ ¡éªŒå’Œåé¦ˆé—­ç¯
- æ”¯æŒå±æœºå¹²é¢„å’Œä¸ªæ€§åŒ–å®šåˆ¶

æ¶æ„ï¼š
1. æƒ…æ„ŸåŒ¹é…å†³ç­–å¼•æ“
2. åŠ¨æ€Promptç”Ÿæˆ
3. å¤šç­–ç•¥å“åº”ç”Ÿæˆï¼ˆè§„åˆ™+ç¼“å­˜+LLMï¼‰
4. æƒ…æ„Ÿä¸€è‡´æ€§æ ¡éªŒ
5. è§’è‰²ç¨³å®šæ€§ç›‘æ§
"""

import yaml
import logging
import random
from typing import Dict, List, Optional, Any, Tuple
from datetime import datetime

from .dynamic_prompt_builder import DynamicPromptBuilder
from backend.utils.sentiment_classifier import SentimentClassifier

logger = logging.getLogger(__name__)


class ResponseGenerator:
    """å“åº”ç”Ÿæˆå™¨ - æ··åˆæ¶æ„"""
    
    def __init__(self, 
                 llm_client,
                 strategy_file: str = "/home/workSpace/emotional_chat/backend/config/emotion_strategy.yaml",
                 enable_consistency_check: bool = True,
                 enable_cache: bool = True):
        """
        åˆå§‹åŒ–å“åº”ç”Ÿæˆå™¨
        
        Args:
            llm_client: å¤§æ¨¡å‹å®¢æˆ·ç«¯ï¼ˆéœ€æ”¯æŒgenerateæ–¹æ³•ï¼‰
            strategy_file: æƒ…æ„Ÿç­–ç•¥é…ç½®æ–‡ä»¶è·¯å¾„
            enable_consistency_check: æ˜¯å¦å¯ç”¨ä¸€è‡´æ€§æ£€æŸ¥
            enable_cache: æ˜¯å¦å¯ç”¨ç¼“å­˜åŒ¹é…
        """
        self.llm_client = llm_client
        self.enable_consistency_check = enable_consistency_check
        self.enable_cache = enable_cache
        
        # åŠ è½½æƒ…æ„Ÿç­–ç•¥
        try:
            with open(strategy_file, 'r', encoding='utf-8') as f:
                self.emotion_strategy = yaml.safe_load(f)
            logger.info(f"âœ“ åŠ è½½æƒ…æ„Ÿç­–ç•¥é…ç½®: {strategy_file}")
        except Exception as e:
            logger.error(f"åŠ è½½ç­–ç•¥é…ç½®å¤±è´¥: {e}")
            self.emotion_strategy = {}
        
        # åˆå§‹åŒ–åŠ¨æ€Promptæ„å»ºå™¨
        self.prompt_builder = DynamicPromptBuilder(self.emotion_strategy)
        
        # åˆå§‹åŒ–æƒ…æ„Ÿä¸€è‡´æ€§åˆ†ç±»å™¨
        if self.enable_consistency_check:
            self.sentiment_classifier = SentimentClassifier()
        
        # ç¼“å­˜çš„å›ºå®šå›å¤ï¼ˆé«˜é¢‘åœºæ™¯ï¼‰
        self.cached_responses = self._load_cached_responses()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            "total_generations": 0,
            "rule_based": 0,
            "cached": 0,
            "llm_generated": 0,
            "consistency_failures": 0,
            "fallback_used": 0
        }
        
        logger.info("âœ“ å“åº”ç”Ÿæˆå™¨å·²åˆå§‹åŒ–")
    
    def generate_response(self,
                         user_input: str,
                         user_emotion: str,
                         user_id: str,
                         emotion_intensity: float = 5.0,
                         conversation_history: Optional[List[Dict]] = None,
                         retrieved_memories: Optional[List[Dict]] = None,
                         user_profile: Optional[Dict] = None,
                         metadata: Optional[Dict] = None) -> Dict[str, Any]:
        """
        ç”ŸæˆAIå›å¤ï¼ˆä¸»å…¥å£ï¼‰
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            user_id: ç”¨æˆ·ID
            emotion_intensity: æƒ…ç»ªå¼ºåº¦(0-10)
            conversation_history: å¯¹è¯å†å²
            retrieved_memories: æ£€ç´¢åˆ°çš„è®°å¿†
            user_profile: ç”¨æˆ·ç”»åƒ
            metadata: é¢å¤–å…ƒæ•°æ®ï¼ˆå¦‚é«˜é£é™©å…³é”®è¯ï¼‰
            
        Returns:
            ç”Ÿæˆç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - response: ç”Ÿæˆçš„å›å¤æ–‡æœ¬
            - generation_method: ç”Ÿæˆæ–¹æ³•ï¼ˆrule/cache/llmï¼‰
            - is_valid: æ˜¯å¦é€šè¿‡ä¸€è‡´æ€§æ£€æŸ¥
            - warnings: è­¦å‘Šä¿¡æ¯
            - metadata: å…ƒæ•°æ®
        """
        self.stats["total_generations"] += 1
        
        result = {
            "response": "",
            "generation_method": "",
            "is_valid": True,
            "warnings": [],
            "metadata": {
                "user_emotion": user_emotion,
                "emotion_intensity": emotion_intensity,
                "timestamp": datetime.now().isoformat()
            }
        }
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºé«˜é£é™©æƒ…å†µï¼ˆå±æœºå¹²é¢„ï¼‰
        if self._is_crisis_situation(user_emotion, metadata):
            response = self._handle_crisis(user_input, user_emotion, metadata)
            result["response"] = response
            result["generation_method"] = "rule_based_crisis"
            result["metadata"]["is_crisis"] = True
            self.stats["rule_based"] += 1
            logger.warning(f"å±æœºå¹²é¢„è§¦å‘ [user={user_id}]: {user_emotion}")
            return result
        
        # 2. ç¼“å­˜åŒ¹é…ï¼ˆé«˜é¢‘å›ºå®šåœºæ™¯ï¼‰
        if self.enable_cache:
            cached_response = self._match_cached_response(user_input, user_emotion)
            if cached_response:
                result["response"] = cached_response
                result["generation_method"] = "cached"
                self.stats["cached"] += 1
                logger.debug(f"ä½¿ç”¨ç¼“å­˜å›å¤ [user={user_id}]")
                return result
        
        # 3. LLMç”Ÿæˆï¼ˆä¸»è¦è·¯å¾„ï¼‰
        try:
            # 3.1 æ„å»ºåŠ¨æ€Prompt
            prompt = self.prompt_builder.build_prompt(
                user_input=user_input,
                emotion=user_emotion,
                emotion_intensity=emotion_intensity,
                conversation_history=conversation_history,
                retrieved_memories=retrieved_memories,
                user_profile=user_profile
            )
            
            # 3.2 è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆ
            raw_response = self._call_llm(prompt)
            
            # 3.3 åå¤„ç†
            processed_response = self._post_process_response(raw_response, user_emotion)
            
            # 3.4 æƒ…æ„Ÿä¸€è‡´æ€§æ ¡éªŒ
            if self.enable_consistency_check:
                is_valid, warnings = self._validate_response(
                    processed_response, 
                    user_emotion,
                    self.emotion_strategy.get(user_emotion, {}).get("tone", "")
                )
                
                if not is_valid:
                    # ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨é™çº§ç­–ç•¥
                    logger.warning(f"ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥: {warnings}")
                    result["warnings"] = warnings
                    self.stats["consistency_failures"] += 1
                    
                    # é™çº§ä¸ºé¢„è®¾å›å¤
                    fallback = self._get_fallback_response(user_emotion)
                    result["response"] = fallback
                    result["generation_method"] = "fallback"
                    result["is_valid"] = False
                    result["metadata"]["original_response"] = processed_response
                    self.stats["fallback_used"] += 1
                    return result
            
            # 3.5 æˆåŠŸç”Ÿæˆ
            result["response"] = processed_response
            result["generation_method"] = "llm_generated"
            result["is_valid"] = True
            self.stats["llm_generated"] += 1
            
        except Exception as e:
            # å¼‚å¸¸å¤„ç†ï¼Œä½¿ç”¨å…œåº•å›å¤
            logger.error(f"LLMç”Ÿæˆå¤±è´¥: {e}")
            result["response"] = self._get_fallback_response(user_emotion)
            result["generation_method"] = "fallback_error"
            result["is_valid"] = False
            result["warnings"].append(f"ç”Ÿæˆå¼‚å¸¸: {str(e)}")
            result["metadata"]["error"] = str(e)
            self.stats["fallback_used"] += 1
        
        return result
    
    def _is_crisis_situation(self, 
                            user_emotion: str, 
                            metadata: Optional[Dict]) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºå±æœºæƒ…å†µ
        
        Args:
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            metadata: å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦ä¸ºå±æœºæƒ…å†µ
        """
        # 1. æƒ…ç»ªç±»å‹åˆ¤æ–­
        if user_emotion == "high_risk_depression":
            return True
        
        # 2. é«˜é£é™©å…³é”®è¯åˆ¤æ–­
        if metadata and metadata.get("requires_crisis_intervention"):
            return True
        
        if metadata and metadata.get("risk_keywords"):
            return True
        
        return False
    
    def _handle_crisis(self, 
                      user_input: str, 
                      user_emotion: str,
                      metadata: Optional[Dict]) -> str:
        """
        å¤„ç†å±æœºæƒ…å†µï¼Œè¿”å›é¢„è®¾çš„å±æœºå¹²é¢„å›å¤
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            metadata: å…ƒæ•°æ®
            
        Returns:
            å±æœºå¹²é¢„å›å¤
        """
        # è·å–å±æœºç­–ç•¥
        crisis_strategy = self.emotion_strategy.get("high_risk_depression", {})
        
        # ä½¿ç”¨é¢„è®¾çš„å±æœºå›å¤
        crisis_response = crisis_strategy.get("fallback", "")
        
        # å¦‚æœæœ‰é…ç½®çš„çƒ­çº¿ä¿¡æ¯ï¼Œå¯ä»¥åŠ¨æ€æ·»åŠ 
        hotlines = crisis_strategy.get("crisis_hotlines", [])
        
        if crisis_response:
            return crisis_response
        else:
            # é»˜è®¤å±æœºå›å¤
            return """æˆ‘éå¸¸å…³å¿ƒä½ ç°åœ¨çš„æƒ…ç»ªçŠ¶æ€ã€‚ä½ ä¸æ˜¯ä¸€ä¸ªäººï¼Œæœ‰å¾ˆå¤šäººæ„¿æ„å¸®åŠ©ä½ ã€‚
å»ºè®®ä½ ç«‹å³è”ç³»å¿ƒç†æ´åŠ©çƒ­çº¿ï¼š
- å¸Œæœ›24çƒ­çº¿ï¼š400-161-9995
- åŒ—äº¬å¿ƒç†å±æœºå¹²é¢„ä¸­å¿ƒï¼š010-82951332
æˆ‘ä¼šä¸€ç›´åœ¨è¿™é‡Œé™ªä½ ã€‚"""
    
    def _match_cached_response(self, 
                               user_input: str, 
                               user_emotion: str) -> Optional[str]:
        """
        åŒ¹é…ç¼“å­˜çš„å›ºå®šå›å¤
        
        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            
        Returns:
            åŒ¹é…çš„å›å¤ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›None
        """
        input_lower = user_input.lower().strip()
        
        # é—®å€™è¯­
        greetings = ["ä½ å¥½", "hi", "hello", "å—¨", "åœ¨å—", "åœ¨ä¸åœ¨"]
        if any(g in input_lower for g in greetings) and len(input_lower) < 10:
            responses = self.cached_responses.get("greeting", [])
            return random.choice(responses) if responses else None
        
        # é“åˆ«è¯­
        farewells = ["å†è§", "æ‹œæ‹œ", "bye", "goodbye", "æ™šå®‰"]
        if any(f in input_lower for f in farewells):
            responses = self.cached_responses.get("goodbye", [])
            return random.choice(responses) if responses else None
        
        # æ„Ÿè°¢è¯­
        thanks = ["è°¢è°¢", "æ„Ÿè°¢", "thanks", "thank you"]
        if any(t in input_lower for t in thanks) and len(input_lower) < 20:
            responses = self.cached_responses.get("thanks", [])
            return random.choice(responses) if responses else None
        
        return None
    
    def _call_llm(self, prompt: str) -> str:
        """
        è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤
        
        Args:
            prompt: å®Œæ•´çš„Prompt
            
        Returns:
            ç”Ÿæˆçš„å›å¤æ–‡æœ¬
        """
        # æ ¹æ®ä¸åŒçš„LLMå®¢æˆ·ç«¯ç±»å‹è°ƒç”¨
        try:
            # å‡è®¾llm_clientæœ‰generateæˆ–predictæ–¹æ³•
            if hasattr(self.llm_client, 'generate'):
                response = self.llm_client.generate(prompt)
            elif hasattr(self.llm_client, 'predict'):
                response = self.llm_client.predict(prompt)
            elif hasattr(self.llm_client, 'invoke'):
                response = self.llm_client.invoke(prompt)
            else:
                # å°è¯•ç›´æ¥è°ƒç”¨
                response = self.llm_client(prompt)
            
            # å¦‚æœè¿”å›çš„æ˜¯å¯¹è±¡ï¼Œæå–æ–‡æœ¬å†…å®¹
            if hasattr(response, 'content'):
                response = response.content
            elif isinstance(response, dict) and 'content' in response:
                response = response['content']
            
            return str(response).strip()
            
        except Exception as e:
            logger.error(f"LLMè°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _post_process_response(self, response: str, user_emotion: str) -> str:
        """
        åå¤„ç†ç”Ÿæˆçš„å›å¤
        
        Args:
            response: åŸå§‹å›å¤
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            
        Returns:
            å¤„ç†åçš„å›å¤
        """
        # 1. å»é™¤é¦–å°¾ç©ºæ ¼å’Œå¤šä½™æ¢è¡Œ
        processed = response.strip()
        
        # 2. å»é™¤å¯èƒ½çš„"å¿ƒè¯­ï¼š"å‰ç¼€ï¼ˆæœ‰äº›æ¨¡å‹ä¼šé‡å¤ï¼‰
        if processed.startswith("å¿ƒè¯­ï¼š") or processed.startswith("å¿ƒè¯­:"):
            processed = processed.split("ï¼š", 1)[-1].split(":", 1)[-1].strip()
        
        # 3. é™åˆ¶é•¿åº¦ï¼ˆæŒ‰å¥å­æ•°ï¼‰
        strategy = self.emotion_strategy.get(user_emotion, {})
        max_sentences = strategy.get("max_length", 3)
        
        # ç®€å•çš„å¥å­åˆ†å‰²ï¼ˆæŒ‰æ ‡ç‚¹ï¼‰
        sentences = []
        for sep in ['ã€‚', 'ï¼', 'ï¼Ÿ', '~', '\n']:
            if sep in processed:
                parts = processed.split(sep)
                sentences.extend([p.strip() + sep for p in parts if p.strip()])
                break
        
        if not sentences:
            sentences = [processed]
        
        # ä¿ç•™å‰Nå¥
        if len(sentences) > max_sentences:
            processed = "".join(sentences[:max_sentences])
        
        # 4. ç¡®ä¿ä¸æš´éœ²AIèº«ä»½
        identity_replacements = {
            "æˆ‘æ˜¯AI": "æˆ‘æ˜¯å¿ƒè¯­",
            "æˆ‘æ˜¯ä¸€ä¸ªAI": "æˆ‘æ˜¯å¿ƒè¯­",
            "ä½œä¸ºAI": "ä½œä¸ºé™ªä¼´è€…",
            "AIåŠ©æ‰‹": "é™ªä¼´è€…",
            "äººå·¥æ™ºèƒ½": "é™ªä¼´è€…"
        }
        for old, new in identity_replacements.items():
            if old in processed:
                processed = processed.replace(old, new)
                logger.warning(f"æ›¿æ¢AIèº«ä»½æš´éœ²è¯: {old} -> {new}")
        
        return processed
    
    def _validate_response(self, 
                          response: str, 
                          user_emotion: str,
                          expected_tone: str) -> Tuple[bool, List[str]]:
        """
        éªŒè¯å›å¤çš„æƒ…æ„Ÿä¸€è‡´æ€§
        
        Args:
            response: ç”Ÿæˆçš„å›å¤
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            expected_tone: æœŸæœ›çš„è¯­æ°”
            
        Returns:
            (is_valid, warnings): æ˜¯å¦æœ‰æ•ˆï¼Œè­¦å‘Šåˆ—è¡¨
        """
        result = self.sentiment_classifier.comprehensive_check(
            ai_response=response,
            user_emotion=user_emotion,
            expected_tone=expected_tone
        )
        
        return result["is_valid"], result["warnings"]
    
    def _get_fallback_response(self, user_emotion: str) -> str:
        """
        è·å–å…œåº•å›å¤
        
        Args:
            user_emotion: ç”¨æˆ·æƒ…ç»ª
            
        Returns:
            å…œåº•å›å¤æ–‡æœ¬
        """
        # ä»ç­–ç•¥ä¸­è·å–fallback
        strategy = self.emotion_strategy.get(user_emotion, {})
        fallback = strategy.get("fallback", "")
        
        if fallback:
            return fallback
        
        # é»˜è®¤å…œåº•å›å¤
        default_fallbacks = {
            "sad": "æˆ‘å¬åˆ°äº†ä½ çš„æ„Ÿå—ï¼Œæˆ‘åœ¨è¿™é‡Œé™ªç€ä½ ã€‚ğŸ’™",
            "anxious": "æˆ‘æ„Ÿå—åˆ°äº†ä½ çš„ç„¦è™‘ã€‚æ·±å‘¼å¸ï¼Œæˆ‘åœ¨è¿™é‡Œé™ªç€ä½ ã€‚ğŸŒ¸",
            "angry": "æˆ‘å¬åˆ°äº†ä½ çš„æ„¤æ€’ã€‚ä½ æœ‰æƒåˆ©è¡¨è¾¾è¿™ç§æ„Ÿå—ã€‚",
            "happy": "å¾ˆé«˜å…´çœ‹åˆ°ä½ è¿™ä¹ˆå¼€å¿ƒï¼ğŸ˜Š",
            "excited": "ä½ çš„å…´å¥‹æ„ŸæŸ“äº†æˆ‘ï¼ç»§ç»­ä¿æŒè¿™ä»½çƒ­æƒ…ï¼âš¡",
            "confused": "æˆ‘ç†è§£ä½ çš„å›°æƒ‘ã€‚æˆ‘ä»¬å¯ä»¥ä¸€èµ·æ…¢æ…¢ç†æ¸…ã€‚ğŸ’­",
            "frustrated": "æˆ‘å¬åˆ°äº†ä½ çš„æ²®ä¸§ã€‚ä½ å·²ç»å¾ˆåŠªåŠ›äº†ã€‚ğŸ’ª",
            "lonely": "æˆ‘åœ¨è¿™é‡Œé™ªç€ä½ ã€‚ä½ å¹¶ä¸å­¤å•ã€‚ğŸ¤—",
            "grateful": "æ„Ÿæ©çš„å¿ƒå¾ˆç¾å¥½ï¼Œè°¢è°¢ä½ çš„åˆ†äº«ã€‚ğŸ™",
            "neutral": "æˆ‘åœ¨è¿™é‡Œå€¾å¬ã€‚å¯ä»¥å¤šè¯´ä¸€äº›å—ï¼ŸğŸ˜Š"
        }
        
        return default_fallbacks.get(user_emotion, "æˆ‘åœ¨è¿™é‡Œå€¾å¬ã€‚è¯·ç»§ç»­è¯´å§ã€‚")
    
    def _load_cached_responses(self) -> Dict[str, List[str]]:
        """
        åŠ è½½ç¼“å­˜çš„å›ºå®šå›å¤
        
        Returns:
            ç¼“å­˜å›å¤å­—å…¸
        """
        global_settings = self.emotion_strategy.get("global_settings", {})
        cached = global_settings.get("cached_responses", {})
        
        # ç¡®ä¿æ¯ä¸ªåœºæ™¯éƒ½æœ‰åˆ—è¡¨
        for key in ["greeting", "goodbye", "thanks"]:
            if key not in cached or not isinstance(cached[key], list):
                cached[key] = []
        
        return cached
    
    def get_statistics(self) -> Dict[str, Any]:
        """
        è·å–ç”Ÿæˆå™¨ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        total = self.stats["total_generations"]
        if total == 0:
            return self.stats
        
        return {
            **self.stats,
            "rule_based_rate": self.stats["rule_based"] / total,
            "cached_rate": self.stats["cached"] / total,
            "llm_rate": self.stats["llm_generated"] / total,
            "failure_rate": self.stats["consistency_failures"] / total,
            "fallback_rate": self.stats["fallback_used"] / total
        }
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
        for key in self.stats:
            self.stats[key] = 0


# ä¾¿æ·å‡½æ•°
def create_response_generator(llm_client, 
                             strategy_file: Optional[str] = None,
                             **kwargs) -> ResponseGenerator:
    """
    åˆ›å»ºå“åº”ç”Ÿæˆå™¨å®ä¾‹
    
    Args:
        llm_client: LLMå®¢æˆ·ç«¯
        strategy_file: ç­–ç•¥æ–‡ä»¶è·¯å¾„
        **kwargs: å…¶ä»–å‚æ•°
        
    Returns:
        ResponseGeneratorå®ä¾‹
    """
    if strategy_file is None:
        strategy_file = "/home/workSpace/emotional_chat/backend/config/emotion_strategy.yaml"
    
    return ResponseGenerator(llm_client, strategy_file, **kwargs)


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    import sys
    sys.path.append("/home/workSpace/emotional_chat")
    
    logging.basicConfig(level=logging.INFO)
    
    # æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯
    class MockLLMClient:
        def predict(self, prompt: str) -> str:
            # ç®€å•çš„æ¨¡æ‹Ÿå›å¤
            if "æ‚²ä¼¤" in prompt:
                return "æˆ‘èƒ½æ„Ÿå—åˆ°ä½ ç°åœ¨çš„ä½è½ã€‚ä½†è¯·ç›¸ä¿¡ï¼Œä½ çš„å­˜åœ¨æœ¬èº«å°±æœ‰ä»·å€¼ã€‚æˆ‘åœ¨è¿™é‡Œï¼Œæ„¿æ„å¬ä½ è¯´æ›´å¤šã€‚ğŸ’™"
            elif "ç„¦è™‘" in prompt:
                return "ç„¦è™‘çš„æ„Ÿè§‰ç¡®å®ä¸å¥½å—ã€‚æ·±å‘¼å¸ï¼Œè®©æˆ‘ä»¬ä¸€æ­¥æ­¥æ¥ã€‚æˆ‘åœ¨è¿™é‡Œé™ªç€ä½ ã€‚ğŸŒ¸"
            else:
                return "æˆ‘åœ¨è¿™é‡Œå€¾å¬ã€‚ä½ æƒ³è¯´ä»€ä¹ˆéƒ½å¯ä»¥ã€‚ğŸ˜Š"
    
    # åˆ›å»ºç”Ÿæˆå™¨
    mock_client = MockLLMClient()
    generator = ResponseGenerator(mock_client)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "user_input": "æˆ‘ä»Šå¤©è¢«é¢†å¯¼æ‰¹è¯„äº†ï¼Œè§‰å¾—è‡ªå·±ä¸€æ— æ˜¯å¤„",
            "user_emotion": "sad",
            "emotion_intensity": 7.5
        },
        {
            "user_input": "æ˜å¤©è¦é¢è¯•ï¼Œæˆ‘å¥½ç´§å¼ ",
            "user_emotion": "anxious",
            "emotion_intensity": 6.0
        },
        {
            "user_input": "ä½ å¥½",
            "user_emotion": "neutral",
            "emotion_intensity": 3.0
        }
    ]
    
    print("\n===== å“åº”ç”Ÿæˆæµ‹è¯• =====\n")
    for i, test in enumerate(test_cases, 1):
        print(f"æµ‹è¯• {i}:")
        print(f"ç”¨æˆ·è¾“å…¥: {test['user_input']}")
        print(f"æƒ…ç»ª: {test['user_emotion']} (å¼ºåº¦: {test['emotion_intensity']})")
        
        result = generator.generate_response(
            user_input=test['user_input'],
            user_emotion=test['user_emotion'],
            user_id="test_user",
            emotion_intensity=test['emotion_intensity']
        )
        
        print(f"ç”Ÿæˆæ–¹æ³•: {result['generation_method']}")
        print(f"AIå›å¤: {result['response']}")
        print(f"æœ‰æ•ˆæ€§: {'âœ“' if result['is_valid'] else 'âœ—'}")
        if result['warnings']:
            print(f"è­¦å‘Š: {', '.join(result['warnings'])}")
        print()
    
    # ç»Ÿè®¡ä¿¡æ¯
    print("===== ç»Ÿè®¡ä¿¡æ¯ =====")
    stats = generator.get_statistics()
    for key, value in stats.items():
        print(f"{key}: {value}")

